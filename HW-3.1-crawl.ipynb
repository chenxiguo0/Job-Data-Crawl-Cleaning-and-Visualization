{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "format:\n",
    "    html:\n",
    "        embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "In this portion of the homework, we will be crawling google jobs to collect various job-descriptions for later processing. \n",
    "\n",
    "We will be using the `serpapi` API to crawl google jobs. `serpapi` is a paid API, but they have a free tier which should be more than enough for this homework. The API allows you to search google programatically, which has a wealth of practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need and API key from SerpApi, it is free to sign up, you shouldn't enter any payment information.\n",
    "\n",
    "https://serpapi.com/manage-api-key\n",
    "\n",
    "We will use the following python wrapper for the API, it can be installed with\n",
    "\n",
    "`pip install google-search-results`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are additional useful reference resources \n",
    "\n",
    "For instructions on the API see the following\n",
    "\n",
    "* [https://serpapi.com/google-jobs-api](https://serpapi.com/google-jobs-api)\n",
    "* [https://serpapi.com/blog/scrape-google-jobs-organic-results-with-python/](https://serpapi.com/blog/scrape-google-jobs-organic-results-with-python/)\n",
    "* [https://serpapi.com/integrations/python](https://serpapi.com/integrations/python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter code \n",
    "\n",
    "Here is some starter code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note: uule parameter`\n",
    "\n",
    "The uule parameter is an encoded location parameter used in Google search queries. It stands for \"Unique User Location Encoding\" and is used to specify the geographic location from which the search is being conducted. This can influence the search results to be more relevant to the specified location.\n",
    "\n",
    "This can be set to `'w+CAIQICINVW5pdGVkIFN0YXRlcw'`, which is an encoded string representing a specific location, i.e. the United States. This encoding helps simulate searches as if they are being conducted from that location, which can be useful for testing or gathering location-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your API key in a centralized location, e.g. `~/.api-keys.json`\n",
    "\n",
    "Read it in with `import json` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../.api-keys.json') as f:\n",
    "    keys = json.load(f)\n",
    "API_KEY = keys['serpapi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful, don't run this too many times for debugging and prototyping, or you will use up all your free searches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = 'data science'\n",
    "params = {\n",
    "\t'api_key':API_KEY,                          # https://serpapi.com/manage-api-key\n",
    "\t'uule': 'w+CAIQICINVW5pdGVkIFN0YXRlcw',\t\t# encoded location (USA)\n",
    "\t'q': search_query,              \t\t\t# search query\n",
    "    'hl': 'en',                         \t\t# language of the search\n",
    "    'gl': 'us',                         \t\t# country of the search\n",
    "\t'engine': 'google_jobs',\t\t\t\t\t# SerpApi search engine\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do one search and explore the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearch(params)   \t\t\t# where data extraction happens on the SerpApi backend\n",
    "result_dict = search.get_dict() \t\t\t# JSON -> Python dict\n",
    "\n",
    "if 'error' in result_dict:\n",
    "    print(\"ERROR FOUND IN SEARCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in result_dict['jobs_results']:\n",
    "    print(result)\n",
    "    # google_jobs_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In web crawling, pagination involves retrieving data across multiple pages of a website. It helps manage large datasets by fetching a limited number of results per request, enabling efficient data extraction without overwhelming the server or exceeding resource limits.\n",
    "\n",
    "You could use pagination to get more results for a given search, however you need to start where the last search left off.\n",
    "\n",
    "You can do this by adding the `next_page_token` to the `params` dictionary.\n",
    "\n",
    "You get the last `next_page_token` from the `result_dict[\"serpapi_pagination\"]`\n",
    "\n",
    "If you don't do pagination, and search 10 times, you will just get the first 10 results over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'next_page_token': 'eyJmYyI6IkVvd0ZDc3dFUVVwSE9VcHJUekJxV0MxQ1FrbHViVXB3UkcxSVlYUmFaR2RUWkhSU1ZFRm5ja1l5TlhGVmRsOTVhVTlGZEROS09GRTRZbW90VkVSWWNHaFVhRlJQZVRsaVdteFNVMnR5WlVaclJuWjNTbk13YWxneVRVNUlSeko2WXkxQ2IzZEhTVVZCVkV4MWNFOW1OR0ozWTE5dlpUTkRlV0pCZFdOME5sWlJaamgzV25OZlkxVTNkVmhoUlRsVE5sRnBZakk1YTNCTk1WSlVla3g0VGpRd1RqWTVXWGxNU2t0SE5uSnJkRVJJVEUxTVNIQnJXWFExYld3MGRWaFljVFpqTFRFNWJXeHlaRGR6UlRSUmIzaHZUalptVTNaZlpVbFphVVY0UlhWdVNrSmpjVlp5UmpJM1YyMXhaVTFYTjFvM2VGZHVZa2hTVmxrM05uUXhjbTVOTlZKNllYbHRiVlJDY1ROTmJUSnllazF5WVRKbFoybzNhR05uYlUxT1RVcFZNR05RVG5ReFR6QjBiRUpFTVdwS2VUaGxiVTlRV1hOWVVVc3dObTQyZDBJek9WWXlOMEpmTWxsb01EUmxaRmRQZEVKbU5tVTBZbmxsV2xaclRXZElRV2x1VDNkelVYSnhOelZ3Y0ZkVFZGVnZNMjFMUmkxS1VuVmhRVXBDWTNreGFsTjRZM2hmYzJsUWVFTkdaemw0U2pkVk0yTmlVMmRIU1VJNE1HRm9Wa2M0Tm14S1ZrZFZUV3R4Y3pkaFFVTk1NazVyYkRGNFIwOW5VVEpuTmtkNVkxTk5OV2MyU0hab2RDMWthMmgyVDFWeWNtRnVRMU5RYW05V0xYaFdTMlphY2s5YWJIWjZhV2hJVTJSSVR6WkxjM0ZFZVhJemJGWkpVVEpuUldGMlVuVlpjVTkzTVdOMU9ISlJaa001VjFremVpMUxkMFpGTnpWeFVVbzRZMGhtVm1GWU4zRkxlakkxVTJweFoxRTVZM2hUY1VkUFRVOW1SM2Q2VUVSa1gzQnVTMll5YXpaT01GUkhZVGxCZUV4d0VoZGxkR04xV2pseExVRjBaVmQzWW10UWMwdDZPSEZCVVJvaVFVWllja1ZqYnpCMlFURk9SRkJxTjFsNlkzcHliRmxrVEY5TE5YWkhTbll4WnciLCJmY3YiOiIzIn0=', 'next': 'https://serpapi.com/search.json?engine=google_jobs&gl=us&google_domain=google.com&hl=en&next_page_token=eyJmYyI6IkVvd0ZDc3dFUVVwSE9VcHJUekJxV0MxQ1FrbHViVXB3UkcxSVlYUmFaR2RUWkhSU1ZFRm5ja1l5TlhGVmRsOTVhVTlGZEROS09GRTRZbW90VkVSWWNHaFVhRlJQZVRsaVdteFNVMnR5WlVaclJuWjNTbk13YWxneVRVNUlSeko2WXkxQ2IzZEhTVVZCVkV4MWNFOW1OR0ozWTE5dlpUTkRlV0pCZFdOME5sWlJaamgzV25OZlkxVTNkVmhoUlRsVE5sRnBZakk1YTNCTk1WSlVla3g0VGpRd1RqWTVXWGxNU2t0SE5uSnJkRVJJVEUxTVNIQnJXWFExYld3MGRWaFljVFpqTFRFNWJXeHlaRGR6UlRSUmIzaHZUalptVTNaZlpVbFphVVY0UlhWdVNrSmpjVlp5UmpJM1YyMXhaVTFYTjFvM2VGZHVZa2hTVmxrM05uUXhjbTVOTlZKNllYbHRiVlJDY1ROTmJUSnllazF5WVRKbFoybzNhR05uYlUxT1RVcFZNR05RVG5ReFR6QjBiRUpFTVdwS2VUaGxiVTlRV1hOWVVVc3dObTQyZDBJek9WWXlOMEpmTWxsb01EUmxaRmRQZEVKbU5tVTBZbmxsV2xaclRXZElRV2x1VDNkelVYSnhOelZ3Y0ZkVFZGVnZNMjFMUmkxS1VuVmhRVXBDWTNreGFsTjRZM2hmYzJsUWVFTkdaemw0U2pkVk0yTmlVMmRIU1VJNE1HRm9Wa2M0Tm14S1ZrZFZUV3R4Y3pkaFFVTk1NazVyYkRGNFIwOW5VVEpuTmtkNVkxTk5OV2MyU0hab2RDMWthMmgyVDFWeWNtRnVRMU5RYW05V0xYaFdTMlphY2s5YWJIWjZhV2hJVTJSSVR6WkxjM0ZFZVhJemJGWkpVVEpuUldGMlVuVlpjVTkzTVdOMU9ISlJaa001VjFremVpMUxkMFpGTnpWeFVVbzRZMGhtVm1GWU4zRkxlakkxVTJweFoxRTVZM2hUY1VkUFRVOW1SM2Q2VUVSa1gzQnVTMll5YXpaT01GUkhZVGxCZUV4d0VoZGxkR04xV2pseExVRjBaVmQzWW10UWMwdDZPSEZCVVJvaVFVWllja1ZqYnpCMlFURk9SRkJxTjFsNlkzcHliRmxrVEY5TE5YWkhTbll4WnciLCJmY3YiOiIzIn0%3D&q=data+science&uule=w%2BCAIQICINVW5pdGVkIFN0YXRlcw'} \n",
      "\n",
      "eyJmYyI6IkVvd0ZDc3dFUVVwSE9VcHJUekJxV0MxQ1FrbHViVXB3UkcxSVlYUmFaR2RUWkhSU1ZFRm5ja1l5TlhGVmRsOTVhVTlGZEROS09GRTRZbW90VkVSWWNHaFVhRlJQZVRsaVdteFNVMnR5WlVaclJuWjNTbk13YWxneVRVNUlSeko2WXkxQ2IzZEhTVVZCVkV4MWNFOW1OR0ozWTE5dlpUTkRlV0pCZFdOME5sWlJaamgzV25OZlkxVTNkVmhoUlRsVE5sRnBZakk1YTNCTk1WSlVla3g0VGpRd1RqWTVXWGxNU2t0SE5uSnJkRVJJVEUxTVNIQnJXWFExYld3MGRWaFljVFpqTFRFNWJXeHlaRGR6UlRSUmIzaHZUalptVTNaZlpVbFphVVY0UlhWdVNrSmpjVlp5UmpJM1YyMXhaVTFYTjFvM2VGZHVZa2hTVmxrM05uUXhjbTVOTlZKNllYbHRiVlJDY1ROTmJUSnllazF5WVRKbFoybzNhR05uYlUxT1RVcFZNR05RVG5ReFR6QjBiRUpFTVdwS2VUaGxiVTlRV1hOWVVVc3dObTQyZDBJek9WWXlOMEpmTWxsb01EUmxaRmRQZEVKbU5tVTBZbmxsV2xaclRXZElRV2x1VDNkelVYSnhOelZ3Y0ZkVFZGVnZNMjFMUmkxS1VuVmhRVXBDWTNreGFsTjRZM2hmYzJsUWVFTkdaemw0U2pkVk0yTmlVMmRIU1VJNE1HRm9Wa2M0Tm14S1ZrZFZUV3R4Y3pkaFFVTk1NazVyYkRGNFIwOW5VVEpuTmtkNVkxTk5OV2MyU0hab2RDMWthMmgyVDFWeWNtRnVRMU5RYW05V0xYaFdTMlphY2s5YWJIWjZhV2hJVTJSSVR6WkxjM0ZFZVhJemJGWkpVVEpuUldGMlVuVlpjVTkzTVdOMU9ISlJaa001VjFremVpMUxkMFpGTnpWeFVVbzRZMGhtVm1GWU4zRkxlakkxVTJweFoxRTVZM2hUY1VkUFRVOW1SM2Q2VUVSa1gzQnVTMll5YXpaT01GUkhZVGxCZUV4d0VoZGxkR04xV2pseExVRjBaVmQzWW10UWMwdDZPSEZCVVJvaVFVWllja1ZqYnpCMlFURk9SRkJxTjFsNlkzcHliRmxrVEY5TE5YWkhTbll4WnciLCJmY3YiOiIzIn0=\n"
     ]
    }
   ],
   "source": [
    "print(result_dict[\"serpapi_pagination\"],\"\\n\")\n",
    "print(result_dict[\"serpapi_pagination\"][\"next_page_token\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do one more search, but this time with pagination, starting where the last search left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = 'data science'\n",
    "params = {\n",
    "\t'api_key':API_KEY,                          # https://serpapi.com/manage-api-key\n",
    "\t'uule': 'w+CAIQICINVW5pdGVkIFN0YXRlcw',\t\t# encoded location (USA)\n",
    "\t'q': search_query,              \t\t\t# search query\n",
    "    'hl': 'en',                         \t\t# language of the search\n",
    "    'gl': 'us',                         \t\t# country of the search\n",
    "    \"num\": 10,\t\t\t\t\t\t\t\t\t# number of results per page\n",
    "\t'engine': 'google_jobs',\t\t\t\t\t# SerpApi search engine\n",
    "    'next_page_token': result_dict[\"serpapi_pagination\"][\"next_page_token\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearch(params)   \t\t\t# where data extraction happens on the SerpApi backend\n",
    "result_dict = search.get_dict() \t\t\t# JSON -> Python dict\n",
    "\n",
    "if 'error' in result_dict:\n",
    "    print(\"ERROR FOUND IN SEARCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in result_dict['jobs_results']:\n",
    "    print(result)\n",
    "    # google_jobs_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility function\n",
    "\n",
    "Create utility function to search google jobs, and save the results to a file.\n",
    "\n",
    "Here is one sketch of what the function might look like:\n",
    "\n",
    "- Imports the current date and time using `datetime`.\n",
    "- Defines `search_google_jobs` to perform a Google Jobs search with a default or custom query.\n",
    "- Accepts parameters for the search query, pagination token, and verbosity.\n",
    "- Sets search parameters like API key, location, language, and search engine.\n",
    "- Appends the pagination token if provided.\n",
    "- Creates a timestamped output filename based on the query and time.\n",
    "- Does a search and data extraction.\n",
    "- Optionally prints the data if `verbose` is `True` and saves results to a JSON file.\n",
    "- Returns the `next_page_token` for pagination or handles errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE\n",
    "import datetime\n",
    "\n",
    "def search_google_jobs(search_query='data science', pagination_token=None, verbose=False):\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    params = {\n",
    "        'api_key':API_KEY,                          \n",
    "        'uule': 'w+CAIQICINVW5pdGVkIFN0YXRlcw',\t\t\n",
    "        'q': search_query,              \t\t\t\n",
    "        'hl': 'en',                         \t\t\n",
    "        'gl': 'us',                         \t\t\n",
    "        \"num\": 10,\t\t\t\t\t\t\t\t\n",
    "        'engine': 'google_jobs',\t\t\t\t\n",
    "    }\n",
    "    \n",
    "    if pagination_token:\n",
    "        params['next_page_token'] = pagination_token\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    result_dict = search.get_dict()\n",
    "    \n",
    "    if 'error' in result_dict:\n",
    "        print(\"ERROR FOUND IN SEARCH:\", result_dict['error'])\n",
    "        return None, None\n",
    "    \n",
    "    if verbose:\n",
    "        print(json.dumps(result_dict, indent=2))  \n",
    "    \n",
    "    next_page_token = None\n",
    "    if 'serpapi_pagination' in result_dict and 'next_page_token' in result_dict['serpapi_pagination']:\n",
    "        next_page_token = result_dict['serpapi_pagination']['next_page_token']\n",
    "        return result_dict, next_page_token\n",
    "\n",
    "    output_filename = f\"googlejobs_{search_query.replace(' ', '_')}_{current_time}.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, next_page_token = search_google_jobs(search_query=\"machine learning engineer\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eyJmYyI6IkVxSUZDdUlFUVVwSE9VcHJUVVZHZW1OTldqQmZVVTR4YUc5RE9VaFNhVVZxT0hsaVltZFdUMHhVT1VSMFRtSm1RbUZYUVdaclJ5MUlZbll4U1ZFMVdFWm1SVWxRUkUweVNHNU1jMmhQYVRaVFVFMWhhRWRHUnpaNGJWWktTbGRKZVdWVGQzcHlVak15VEhscmNtaG5ZM2hSYzNsbVdEZEZkV2g1WkhZMmVERkljMjEyZFVneGRVWkhZVlF4U0dKNVVIaFdiV1ZJTjJsTFJreG1OMUl0ZFcwNFQwaE9MWGxMV201UlpXZ3ROV1ZmZGtwYWJTMVlNVUpIY2poeVpXOWFZMjR6UkZKRk1FZEhWelJWTlY5RVZHdEtXSEJJZUMwNVZXOUNRVm80TjFKcVgyVjBlR2xQTkd0UVJHVkdiek4wTTI5S05FMXlZV1J1WDJOQlJIRXdhV3BDV1RaV1MybG1USGxUWW1jMk0yaGhUVWxsTm14eFdWVkNUbWs0TTNoa1IwRTBlRzVvZDFSa2FsWkNWRTFGWlZsa1VHYzBXWE5GVTI4MGExZFNUVmQyYVhKd05FSkdPRXBDTm1SNWNqWTBjWEZHZEdsNlNtMWxhSGcxWlZrMWN6bDVla3RJY2toaGVHaFJja3RPV1ZvMVgzWXpPRkUzYVhkWlpucFliR3czTms5NU5tMVZXbTVJYTNOMFdVbG5VVXhqYVV3MmRVeHNVM2MwZVZJd09IZG9OR0phVFZkbGFtUlBjV0pVWkZZMldESjRka1J4ZUhsdmVWOUZNME5LWDNFdFp6aHNaMHB4VEhCbVNGZEpkMWxFYkV0U01qbE9iWHBHWlhKb1ZsUm9iMTlwWkd4SlpuaGxaMFUyWW1aRFRGSTFhbWd5YmpsT2NtdE5iMnRyUVY5cVkybG9iVmRUYmxaNmJFMTFNa05ZY21wU1lrdGxTV04yVmtWMFVXbElWbDkxU0RoQlRGQlJNelZMV1dKTE1FNXJRbWRLUTFreU4ybEplbUpvZURaNWRYb3dZMEZuUWxkMFpGRnhaVUV6Vmt0SVoyeERSbmN0VkhCUVl6bHBXV3BFUW1GbWRtMDJVUklYWm1SamRWbzFlbFJDVG1vdGQySnJVRGhtZVdselFYY2FJa0ZHV0hKRlkzQXlOamRxTFZORVNtaGZkV1p6Y0hseWNVdFpOakF0V2kxdGJWRSIsImZjdiI6IjMifQ=='"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_page_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_google_jobs(search_query=\"machine learning engineer\", pagination_token=next_page_token, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manager Data Scientist Capital One\n",
      "Senior Data Scientist, ASE iCloud Data Organization [Executive Communications] Apple\n",
      "Data Scientist Harris-Stowe State University\n",
      "Senior Computational Biology - Real World Data Science Tempus\n",
      "Lead Data Scientist Cox Communications\n",
      "Senior Data Scientist / Machine Learning Engineer - GenAI & LLM Databricks\n",
      "Sr. data science (Artificial Intelligence) Pyromis\n",
      "Data Scientist, Paramount Advertising Paramount\n",
      "Senior Scientist, Machine Learning 50056740 - Senior Scientist\n",
      "BP Data Science Analyst - NY, NJ, Or PA Visions Federal Credit Union\n"
     ]
    }
   ],
   "source": [
    "for result in result_dict['jobs_results']:\n",
    "    print(result[\"title\"], result[\"company_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over job titles\n",
    "\n",
    "These titles reflect a wide range of roles that leverage data science and machine learning skills in various industries and specialties.\n",
    "\n",
    "- Data Scientist\n",
    "- Machine Learning Engineer\n",
    "- Artificial Intelligence Specialist\n",
    "- Data Analyst\n",
    "- Business Intelligence Analyst\n",
    "- Research Scientist (AI/ML)\n",
    "- Deep Learning Engineer\n",
    "- NLP Engineer (Natural Language Processing)\n",
    "- Computer Vision Engineer\n",
    "- Data Engineer\n",
    "- Applied Scientist\n",
    "- Quantitative Analyst (Quant)\n",
    "- Predictive Modeler\n",
    "- AI Solutions Architect\n",
    "- Statistician\n",
    "- Big Data Engineer\n",
    "- Data Science Consultant\n",
    "- Automation Engineer\n",
    "- Analytics Manager\n",
    "- Decision Scientist\n",
    "- Operations Research Analyst\n",
    "- Robotics Engineer\n",
    "- Bioinformatics Data Scientist\n",
    "- Healthcare Data Analyst\n",
    "- Financial Data Scientist\n",
    "- Customer Insights Analyst\n",
    "- Marketing Data Analyst\n",
    "- Data Strategy Manager\n",
    "- Cloud AI Engineer\n",
    "- Computational Scientist\n",
    "- Fraud Detection Specialist\n",
    "- Risk Analyst\n",
    "- Data Architect\n",
    "- Algorithm Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each keyword, do three searches, using pagination, this will result in around 30 jobs per keyword (assuming there are at least 30 jobs for the particular keyword), save each search results to a file. \n",
    "\n",
    "Note, just to be safe, wait a one second between each request e.g. using `time.sleep(1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "job_titles = [\n",
    "    \"Data Scientist\",\n",
    "    \"Machine Learning Engineer\",\n",
    "    \"Artificial Intelligence Specialist\",\n",
    "    \"Data Analyst\",\n",
    "    \"Business Intelligence Analyst\",\n",
    "    \"Research Scientist (AI-ML)\",\n",
    "    \"Deep Learning Engineer\",\n",
    "    \"NLP Engineer (Natural Language Processing)\",\n",
    "    \"Computer Vision Engineer\",\n",
    "    \"Data Engineer\",\n",
    "    \"Applied Scientist\",\n",
    "    \"Quantitative Analyst (Quant)\",\n",
    "    \"AI Solutions Architect\",\n",
    "    \"Statistician\",\n",
    "    \"Big Data Engineer\",\n",
    "    \"Data Science Consultant\",\n",
    "    \"Automation Engineer\",\n",
    "    \"Analytics Manager\",\n",
    "    \"Operations Research Analyst\",\n",
    "    \"Robotics Engineer\",\n",
    "    \"Bioinformatics Data Scientist\",\n",
    "    \"Financial Data Scientist\",\n",
    "    \"Customer Insights Analyst\",\n",
    "    \"Marketing Data Analyst\",\n",
    "    \"Data Strategy Manager\",\n",
    "    \"Cloud AI Engineer\",\n",
    "    \"Computational Scientist\",\n",
    "    \"Fraud Detection Specialist\",\n",
    "    \"Risk Analyst\",\n",
    "    \"Data Architect\"\n",
    "]\n",
    "\n",
    "print(len(job_titles)*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now insert code to iterate over the job titles, and perform the searches.\n",
    "\n",
    "Be very careful, this needs to be 100% correct before running it, otherwise you will burn through your free searches.\n",
    "\n",
    "I would recommend doing just one iteration of the loop as a trial run, if that looks good, then do do the next iteration and carefully check the results, if everything looks good then do remaining 28 iterations.\n",
    "\n",
    "Note: sometimes the Pagination will return less than 10 results, so you may end up with slightly less than 30 results per keyword, e.g. 25 to 30\n",
    "\n",
    "Remember to clean the job tiles to remove any characters like spaces, `/` or `()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE\n",
    "all_results = []\n",
    "\n",
    "for title in job_titles: \n",
    "    page_count = 0\n",
    "    pagination_token = None\n",
    "\n",
    "    while page_count < 3:\n",
    "        result_dict, next_token = search_google_jobs(search_query=title, pagination_token=pagination_token, verbose=True)\n",
    "        \n",
    "        if result_dict is None:\n",
    "            print(f\"Search failed for {title}\")\n",
    "            break\n",
    "\n",
    "        page_count += 1\n",
    "\n",
    "        all_results.append({\n",
    "        \"query\": title,              \n",
    "        \"page\": page_count + 1,      \n",
    "        \"results\": result_dict       \n",
    "        })\n",
    "\n",
    "        if not next_token:\n",
    "            print(f\"No more pages for {title}\")\n",
    "            break\n",
    "        \n",
    "        pagination_token = next_token\n",
    "    \n",
    "        \n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_filename = f\"googlejobs_alltitles_{current_time}.json\"\n",
    "    \n",
    "with open(output_filename, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
